# 一、监督学习

## 1. Regression线性回归:

### 定义

Predicet continuous valued output(price)

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210101150716375.png" alt="image-20210101150716375" style="zoom:50%;" />

### 处理过程

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210101153224538.png" alt="image-20210101153224538" style="zoom:50%;" />

### 代价函数（Cost function）

> 目的：减少损失，使预测值和实际值的差距之和尽可能的小

* 拟合成直线的情况（初学适用）

  <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210101154621983.png" alt="image-20210101154621983" style="zoom:70%;" />
  
* 多特征情况下
  $$
  h_\theta(x)=\theta^Tx
  $$

  > 此时x和θ都是列向量，通常令x0 = 1

### 梯度下降Gradent descent algorithm

> 目的：最小化函数，包括但不限于代价函数，实际上是选择合适的参数使函数最小化

* 步骤

  > 1. 给定初始参数θ0，θ1
  >2. 变化θ0，θ1，使J(θ0，θ1)减小到我们期望的最小值
  
  * 公式表示
  
    repeat until convergence{
    $$
    \theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)
    $$
    }
  
    > α表示变化速度(learning rate)，表示跨多大步子，永远是一个正数
    >
    > 对于θ0和θ1需要同时变化(simultaneous update)
    >
    > <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210101162104427.png" alt="image-20210101162104427" style="zoom:70%;" />
  
* 分析

  * 可能会达到局部最优的情况
  * 越接近极值时，由于倒数越来越接近于0，那么跨的步子会越来越小 
  * 在每次循环后，都要遍历全部数据集，计算此时的代价函数

* 多特征情况下，此时代价函数可以表示为：
  $$
  \theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}{(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}}
  $$

  > 其中上标i表示第i组数据向量，下标j表示这组数量向量中的第j个元素

### 特征缩放

* 应用场景：

  当特征值的数值范围差距过大时，需要做类似归一化的操作，使得特征值的参数范围差不多，这样可以使得在计算梯度时下降得更直接，更快

### 正规方程

* 应用场景：

  由于梯度下降是找寻代价函数得极小值，需要不断迭代计算，可能会花费较多时间

* 解决方法：

  可以用微积分中的逐项偏导然后令其为0，然后综合计算求出各项特征值的数值

* 公式

  <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210101174618439.png" alt="image-20210101174618439" style="zoom:80%;" />

  其中适合的θ值为
  $$
  \theta = (X^TX)^{-1}X^Ty
  $$

* 与梯度下降的区别

  通俗来说，梯度下降是将结果慢慢的**试出来**，而正规方程是将结果**算出来**

  如果特征值n较大，偏向于选择梯度下降；如果特征值n较小，那么偏向于选择正规方程




## 2. Classification

> Discrete valued output(0 or 1)

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210101150801298.png" alt="image-20210101150801298" style="zoom:50%;" />

### Logistic Regression

* Logistic Function
  $$
  g(z) = \frac{1}{1+e^{-z}}
  $$

* Logistic Model

  > want 0<h(x)<1，则需要对h(x)做一定的变换操作

  $$
  h_\theta(x)=g(\theta^Tx)
  $$

  其中g()是Logistic Function，这样就可以将h(x)限制在[0,1]

* 分析

  <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102131706356.png" alt="image-20210102131706356" style="zoom:80%;" />

  此时可以假设

  > 当h(x)≥0.5时，即当θTx≥0时，预测y=1
  >
  > 当h(x)<0.5时，即当θTx<0时，预测y=0

  这样可以达到分类的目的

* 决策界限(Decision Boundary)

  > 不同分类之间的界限

  * 示例1：

  <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102133422849.png" alt="image-20210102133422849" style="zoom:80%;" />

  * 示例2：

  <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102133812748.png" alt="image-20210102133812748" style="zoom:80%;" />

* 代价函数Cost Function

  > 在训练模型时需要用到

  $$
  J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})
  \\
  Cost(h_\theta(x),y) = \begin{cases} -\log(h_\theta(x)),\ \ \ \ \ \ \ \ \ if\ y=1 \\-\log(1-h_\theta(x)),\ \ if\ y=0 \end{cases}
  \\
  $$

  ​	其中Cost(h(x),y)还可以写成如下形式
  $$
  Cost(h_\theta(x),y)=-y\log(h_\theta(x))-(1-y)\log(1-h_\theta(x))
  $$
  说明：

  > 1. h(x),即1/[1+e^-z]值域为(0,1)，(0,1)同时作为cost(h(x),y)的定义域
  > 2. 以拟合y=1举例，Cost(h(x),y)取-log(h(x))
  > 3. 当h(x)=1时，-log(1)=0,h(x)=0,-log(0)趋于正无穷
  > 4. -log(x)在(0,1)内单调递减，不存在局部最小值，合理的利用**梯度下降**一定能够收敛到全局最小值
  > 5. 我们的期望是，当y=1，预测到h(x)=1时，代价为0；当y=0，预测到h(x)=1时，代价为无穷大，那么这个代价函数可以实现此目标
  > 6. 最终目的是选取合适的θ（权值），用这个代价函数取拟合很合适

  举例：

  > if y=1
  >
  > <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102135629906.png" alt="image-20210102135629906" style="zoom:80%;" />
  >
  > 达到效果：当越接近1时，代价越小

  > if y=0
  >
  > <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102140310638.png" alt="image-20210102140310638" style="zoom:80%;" />
  >
  > 达到效果：当越接近0，代价越小

* 梯度下降求解

  Gredient Descent
  $$
  J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}{y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log(1-h_\theta(x^{(i)}))}
  $$
  Want min J(θ)

  ​	Repeat {
  $$
  \theta_j:=\theta_j-\alpha\sum_{i=1}^{m}{(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}}
  $$
  ​	}

  Algorithem looks identical to linear regression!

  此时线性回归和逻辑回归的区别

  >h(x)有区别,
  >
  >在线性回归中:
  >$$
  >h_\theta(x)=\theta^Tx
  >$$
  >在逻辑回归中:
  >$$
  >h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}
  >$$

### 多元分类

> 不止含有两种类型的数据，甚至是多个，比如晴天、雨天、多云、下雨等多类型

* 解决方法

  > 对每种类型单独做一次分类，其余类型也同样

  <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102153658208.png" alt="image-20210102153658208" style="zoom:80%;" />

* 文字叙述

  <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102153921363.png" alt="image-20210102153921363" style="zoom:80%;" />

  > 比如有三种类型的数据，那么对于每一种类型数据都构建一个分类器，然后保存三个分类器的模型
  >
  > 输入一个x，将x放到三个分类器中计算，得出三个结果，那么结果值最大的就属于该分类

## 3.过度拟合Overfitting

### 定义

> 训练的函数能够非常好的拟合训练集，但是却不能够应用到新的样本上

* 示例1：

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102154812486.png" alt="image-20210102154812486" style="zoom:80%;" />

* 示例2：

  <img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210102155100172.png" alt="image-20210102155100172" style="zoom:80%;" />

### 解决方法

* 减少特征量
  * 手动选择
  * 机器选择（Module selection algorithm）
* 正则化
  * 保留所有特征量，但是减少量级(magnitude)或者参数θ的大小

### 正则化

> 弱化个别θ带来的过拟合影响，加入惩罚函数，弱化θ

* 影响：
  * 简化拟合函数
  * 更少可能的产生过拟合现象
* 代价函数

$$
J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}{(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2}
$$

> 其中最后一项就是用于控制θ的函数项，λ成为正则化参数

* 分析

  >如果正则化参数λ设置过大，那么对于θ的惩罚会过重，就会忽略过多的特征量，会导致欠拟合现象

### 线性回归正则化

* 过程

  Repeat{
  $$
  \theta_0:=\theta_0-\alpha\frac{1}{m}{\sum_{i=1}^{m}{(h_\theta(x^{(i)})-}y^{(i)})x_0^{(i)}}
  \\
  \theta_j:=\theta_j-\alpha\frac{1}{m}{\sum_{i=1}^{m}{(h_\theta(x^{(i)})-}y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j}
  $$
  }

  其中第二个式子还可以结合为
  $$
  \theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}{\sum_{i=1}^{m}{(h_\theta(x^{(i)})-}y^{(i)})x_j^{(i)}}
  $$
  对于正规方程来计算，如果n为2时，可以写为
  $$
  \theta = (X^TX+\lambda\left[ \begin{matrix}0&0&0\\0&1&0\\0&0&1\end{matrix}\right])^{-1}X^Ty
  $$

### 逻辑回归正则化

​	过程和线性回归正则化类似，但是要注意的是，线性回归和逻辑回归中h(x)的表达含义不一样

# 二、非监督学习

## 1. 聚类算法

### 定义

将不同的数据集分成几个簇，并不知道每个数据的具体含义，只是将其分为几个簇

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210101150618129.png" alt="image-20210101150618129" style="zoom:50%;" />

### 应用

* Organize computing clusters
* Social network analysis
* Market segmentation
* Astronomical data analysis
* ...

# 三、神经网络

## 1.概念图

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210106232623477.png" alt="image-20210106232623477" style="zoom:80%;" />

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210106232739630.png" alt="image-20210106232739630" style="zoom:80%;" />



## 2.计算过程

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210106233153217.png" alt="image-20210106233153217" style="zoom:100%;" />

> 稍微分析下：上一层的输出作为下一层的输入，像个套娃
>
> 这里的Θ是3×4矩阵，x是4×1矩阵，所以Θ不用转置

## 3.例子

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210109202015356.png" alt="image-20210109202015356" style="zoom:80%;" />

> 由上述三个单个神经元组成了最后想要的异或结果，类似于电路设计
>
> 第一层是输入层，最后一层是输出层，中间的全部为隐藏层

## 4.代价函数

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210109205232374.png" alt="image-20210109205232374" style="zoom:80%;" />

> 对照Logistic Regression的代价函数比较容易理解
>
> 其中**h(x)**表示输出的是一个K维的向量，**(h(x))_i**是指向量中的第i个
>
> 第一个求和K是指：将k个输出结点的**结合值h(x)**与**真值y**进行比较，相当于逻辑回归中的代价函数执行K次
>
> 最后的连续三个求和是打算对神经网络中的每个权值进行不同的惩罚

## 5.反向传播算法（Backpropagation algorithm）

> 目标：最小化代价函数
>
> 方法：调整神经网络中的权值，使其输出结果尽量接近真值y
>
> 基本思想：
>
> ​	1. 将训练集数据输入到神经网络的输入层，经过隐藏层，最后达到输出层并输出结果，这就是前向传播过程。
>
>  	2. 由于神经网络的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；
>  	3. 在反向传播的过程中，根据误差调整各种参数的值（相连神经元的权重），使得总损失函数减小。
>  	4. 迭代上述三个步骤（即对数据进行反复训练），直到满足停止准则。



### （1）正向传播

> 有训练集(x,y)
>
> 自定义θ，将输入值x输入，看是否能够得到正确的值y
>
> 方向是由输入到输出

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210109210951789.png" alt="image-20210109210951789" style="zoom:80%;" />

### （2）反向传播

#### 理解图



<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210109214654115.png" alt="image-20210109214654115" style="zoom:80%;" />

> 其中δ是每一层的**计算值**与**真值**的插值，简称误差
>
> 从输出项开始，向后推导出每一层的误差，直到输入项，公式如上
>
> 前一层的误差取决于后一层的误差和权值

#### 算法步骤

<img src="C:\Users\Qian\AppData\Roaming\Typora\typora-user-images\image-20210109215638014.png" alt="image-20210109215638014" style="zoom:80%;" />

#### 如何训练

> 1. 选择随机初始化权重
> 2. 正向传播
> 3. 计算代价函数
> 4. 应用反向传播，其中主要是计算J(θ)对θ_i的偏导，最终是达到调整好整个网络的权重
> 5. 进行梯度检查，比较由数学方法（极限逼值）求出的偏导和反向传播求出的偏导，判断是否正常运行
> 6. 将梯度下降算法或者其他优化算法与反向传播结合，尽量去减小代价函数J(θ)的值

# 四、实战

## 1.人工智能发展三要素

1.数据

2.算法

3.计算力

 - CPU和GPU区别

   > cpu主要适合I/O密集型任务
   >
   > GPU主要适合计算密集型任务

## 2.分支

1. 计算机视觉
2. 语音识别
3. 文本挖掘/分类
4. 机器翻译
5. 机器人

## 3.工作流程

1. 获取数据
2. 数据基本处理
3. 特征工程
   1. 特征提取
   2. 特征预处理
   3. 特征降维
4. 机器学习（模型训练）
5. 模型评估
   1. 结果达到要求，上线服务
   2. 没有达到要求，重新上面步骤