# 预处理

## Batch Norm

> 以学习时的mini-batch为单位，进行使数据分布的均值为0、方差为1的正规化

**目的：**

​	网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。

​	我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。==BN的提出，就是要解决在训练过程中，中间层数据分布发生改变的情况。==

**过程：**

1. 正规化

$$
\mu_B \leftarrow \frac{1}{m}\sum_{i=1}^m x_i \\
\sigma^2_B \leftarrow \frac{1}{m}\sum_{i=1}^m(x_i - \mu_B)^2 \\
x_i \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2+\varepsilon}}
$$

2. 缩放平移

$$
y_i \leftarrow \gamma x_i + \beta
$$

> 一开始γ=1，β = 0，然后再通过学习调整到合适的值
>
> 由于归一化的x_i基本会被限制在正态分布下，使得网络的表达能力下降。为解决该问题，我们引入两个新的参数γ、β
>
> γ：尺度变换
>
> β：偏移

**特点：**

1. 可以使学习快速进行（可以增大学习率）
2. 不那么依赖初始值
3. 抑制过拟合（降低Dropout等必要性）



# 激活函数

> 功能：决定如何来激活输入信号的总和（线性->非线性）

## sigmoid

$$
h(x) = \frac{1}{1+e^{-x}}
$$

![image-20220210114413309](D:\tools\typora\Typora-pic\image-20220210114413309.png)

## ReLu

$$
h(x) = 
	\begin{cases}
	x & x>0 \\
	0 & x \leq 0
	\end{cases}
$$

![image-20220210115717447](D:\tools\typora\Typora-pic\image-20220210115717447.png)

**导数：**
$$
\frac{\partial y}{\partial x} = 
	\begin{cases}
	1 & x>0 \\
	0 & x \leq 0
	\end{cases}
$$

## Affine

> 全连接层

$$
np.dot(X,W) + B
$$



**导数：**
$$
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} · W^T \\
\frac{\partial L}{\partial W} = X^T · \frac{\partial L}{\partial Y}
$$


## softmax

> 分子是输入信号的指数函数，分母是所有输入信号的指数函数的和

$$
y_k = \frac{e^{a_k}}{\sum_{i=0}^ne^{a_i}}
$$

**改进点：**

​	因为指数函数可能会产生溢出，所以所有信号需要减去信号中的最大值，然后再参与softmax计算。

**特点：**

​	输出是0.0到1.0之间的实数，且总和为1，所以可以将其输出解释为==概率==

**反向传播操作：**
$$
y_i - t_i
$$


# 损失函数

> 表示神经网络性能的“恶劣程度”

## 均方误差

$$
E = \frac{1}{2}\sum_k(y_k-t_k)^2
$$

> y_k表示神经网络的输出，t_k表示标签数据，k代表数据的维数

## 交叉熵误差

$$
E = -\sum_kt_k\log y_k
$$

> y_k是神经网络的输出，t_k表示标签数据

# 优化器

## 随机梯度下降法 SGD

> 对随机选择的数据进行梯度下降法，随机指随机选择的mini batch数据

**优点：**

​	简单，容易实现

**缺点：**

​	解决某些问题时没有效率

![image-20220210190558546](D:\tools\typora\Typora-pic\image-20220210190558546.png)

## Momentum

**更新：**
$$
v \leftarrow \alpha v - \beta \frac{\partial L}{\partial W} \\
W \leftarrow W + v
$$

> 变量v对应物理上的速度，表示物体在梯度方向上的受力，在这个力的作用下，物理的速度增加这一物理法则

**优点：**

​	减弱SGD带来的“之”字形的变动程度

![image-20220210190614182](D:\tools\typora\Typora-pic\image-20220210190614182.png)

## AdaGrad

> 其中Ada来自英文单词Adaptive，会为参数的每个元素适当地调整学习率

**更新：**
$$
h \leftarrow h+\frac{\partial L}{\partial W} \bigodot \frac{\partial L}{\partial W} \\
W \leftarrow W- \eta \frac{1}{\sqrt h}\frac{\partial L}{\partial W}
$$

> 变量h保存了以前所有梯度值的平方和，然后在更新参数时，通过乘以1/根h，就可以调整学习的尺度。
>
> 意味着，参数的元素中变动较大时，对应的学习率变小，可以达到学习率衰减的效果

**优点：**

​	学习越深入，更新幅度越小

​	函数取值高效地向着最小值移动，“之”字形变动程度有所衰减

![image-20220210192307028](D:\tools\typora\Typora-pic\image-20220210192307028.png)

**缺点：**

​	无止境的学习后，更新两就会变为0，完全不再更新，可通过RMSProp方法改善

## RMSProp

> RMSProp方法并不是将过去的所有梯度一视同仁的相加，而是逐渐遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。
>
> 从专业上来讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度

**更新：**
$$
S_{dW} = \beta S_{dW} + (1-\beta)dW^2 \\
W = W - \alpha \frac{dW}{\sqrt{S_{dW}}}
$$

## Adam

> 为RMSprop和Momentum的结合版，既考虑学习率衰减，又考虑物理规则
>
> 利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率

**更新：**
$$
m_t = \beta_1m_{t-1} + (1-\beta_1)g_{t-1} \\
v_t = \beta_2v_{t-1} + (1-\beta_2)g_{t-1}^2 \\
m_t = \frac{m_t}{1-\beta_1^t} \\
v_t = \frac{v_t}{1-\beta_2^t} \\
\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{v_t}+\varepsilon}m_t
$$

> 推荐值：β1 = 0.9，β2 = 0.999
>
> m为动量，v为自适应学习率
>
> 刚开始所需动量比较大，后面模型基本稳定后，逐步减小对动量的依赖，自适应学习率同样也会随迭代次数逐渐衰减

![image-20220210192948529](D:\tools\typora\Typora-pic\image-20220210192948529.png)

# 初始值

## S曲线函数权重

**Xavier初始值**

> 因为sigmoid和tanh函数左右对称，且中央附近可以视作线性函数，所以适用Xavier初始值

如果前一层的节点数为n，则初始值使用标准差为
$$
\frac{1}{\sqrt{n}}
$$
的分布

## ReLu权重

**He初始值**

如果前一层的节点数为n，则初始值适用标准差为
$$
\sqrt{\frac{2}{n}}
$$
的高斯分布

# 正则化

## 权值衰减

> 对大的权重进行惩罚（很多过拟合原本就是因为i权重参数取值过大引起的）
>
> 损失函数加上权重的平方范数（L2）

$$
\frac{1}{2}\lambda W^2
$$

**优点：**

​	简单、易实现

**缺点：**

​	当网络的模型变得很复杂时，权值衰减难以应对

**作用：**

​	L2正则损失对大数值权重进行惩罚，喜欢分散权值，鼓励分类器将所有维度的特征都用起来，而不是强烈的依赖其中少数几维特征

## Dropout

> 学习的过程中随机删除神经元
>
> 每传递一次数据，就会随机选择要删除的神经元

# 经典模型

## LeNet-5

![image-20220211220026606](D:\tools\typora\Typora-pic\image-20220211220026606.png)

> 激活函数使用的是sigmoid和tanh函数

**代码结构：**

```python
import time
import torch
from torch import nn, optim

import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2), # kernel_size, stride
            nn.Conv2d(6, 16, 5),
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2)
        )
        self.fc = nn.Sequential(
            nn.Linear(16*4*4, 120),
            nn.Sigmoid(),
            nn.Linear(120, 84),
            nn.Sigmoid(),
            nn.Linear(84, 10)
        )

    def forward(self, img):
        feature = self.conv(img)
        output = self.fc(feature.view(img.shape[0], -1))
        return output
```



## AlexNet

**特点：**

1. 成功使用==ReLU==作为CNN的激活函数
2. 训练时使用==Dropout==随机忽略一部分神经元，以避免模型过拟合
3. 在CNN中使用重叠的==最大池化==
4. 使用加入==动量==的小批量梯度下降算法加速了训练过程的收敛
5. 使用GPU训练
6. 数据增强，缓解过拟合

**结构：**

![image-20220215152647375](D:\tools\typora\Typora-pic\image-20220215152647375.png)

**实现：**

```python
# -*- coding=UTF-8 -*-  
import tensorflow as tf  
# 输入数据  
import input_data  
mnist = input_data.read_data_sets("/tmp/data/", one_hot=True)  
# 定义网络超参数  
learning_rate = 0.001  
training_iters = 200000  
batch_size = 64  
display_step = 20  
# 定义网络参数  
n_input = 784  # 输入的维度  
n_classes = 10 # 标签的维度  
dropout = 0.8  # Dropout 的概率  
# 占位符输入  
x = tf.placeholder(tf.types.float32, [None, n_input])  
y = tf.placeholder(tf.types.float32, [None, n_classes])  
keep_prob = tf.placeholder(tf.types.float32)  
# 卷积操作  
def conv2d(name, l_input, w, b):  
    return tf.nn.relu(tf.nn.bias_add( \  
    tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], padding='SAME'),b) \  
    , name=name)  
# 最大下采样操作  
def max_pool(name, l_input, k):  
    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], \  
    strides=[1, k, k, 1], padding='SAME', name=name)  
# 归一化操作  
def norm(name, l_input, lsize=4):  
    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)  
# 定义整个网络   
def alex_net(_X, _weights, _biases, _dropout):  
    _X = tf.reshape(_X, shape=[-1, 28, 28, 1]) # 向量转为矩阵  
    # 卷积层  
    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])  
    # 下采样层  
    pool1 = max_pool('pool1', conv1, k=2)  
    # 归一化层  
    norm1 = norm('norm1', pool1, lsize=4)  
    # Dropout  
    norm1 = tf.nn.dropout(norm1, _dropout)  
   
    # 卷积  
    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])  
    # 下采样  
    pool2 = max_pool('pool2', conv2, k=2)  
    # 归一化  
    norm2 = norm('norm2', pool2, lsize=4)  
    # Dropout  
    norm2 = tf.nn.dropout(norm2, _dropout)  
   
    # 卷积  
    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])  
    # 下采样  
    pool3 = max_pool('pool3', conv3, k=2)  
    # 归一化  
    norm3 = norm('norm3', pool3, lsize=4)  
    # Dropout  
    norm3 = tf.nn.dropout(norm3, _dropout)  
   
    # 全连接层，先把特征图转为向量  
    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]])   
    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1')   
    # 全连接层  
    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') 
    # Relu activation  
    # 网络输出层  
    out = tf.matmul(dense2, _weights['out']) + _biases['out']  
    return out  
   
# 存储所有的网络参数  
weights = {  
    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),  
    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),  
    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),  
    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),  
    'wd2': tf.Variable(tf.random_normal([1024, 1024])),  
    'out': tf.Variable(tf.random_normal([1024, 10]))  
}  
biases = {  
    'bc1': tf.Variable(tf.random_normal([64])),  
    'bc2': tf.Variable(tf.random_normal([128])),  
    'bc3': tf.Variable(tf.random_normal([256])),  
    'bd1': tf.Variable(tf.random_normal([1024])),  
    'bd2': tf.Variable(tf.random_normal([1024])),  
    'out': tf.Variable(tf.random_normal([n_classes]))  
}  
# 构建模型  
pred = alex_net(x, weights, biases, keep_prob)  
# 定义损失函数和学习步骤  
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))  
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)  
# 测试网络  
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))  
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  
# 初始化所有的共享变量  
init = tf.initialize_all_variables()  
# 开启一个训练  
with tf.Session() as sess:  
    sess.run(init)  
    step = 1  
    # Keep training until reach max iterations  
    while step * batch_size < training_iters:  
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)  
        # 获取批数据  
        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})  
        if step % display_step == 0:  
            # 计算精度  
            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})  
            # 计算损失值  
            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})  
            print "Iter " + str(step*batch_size) + ", Minibatch Loss= " + "{:.6f}".format(loss) + ", Training Accuracy= " + "{:.5f}".format(acc)  
        step += 1  
    print "Optimization Finished!"  
    # 计算测试精度  
    print "Testing Accuracy:", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.})  
 
```

## VGG-16

> 16表示在这个网络中包含16个卷积层和全连接层

**结构：**

![image-20220211220631607](D:\tools\typora\Typora-pic\image-20220211220631607.png)

![image-20220215155401012](D:\tools\typora\Typora-pic\image-20220215155401012.png)

**特点：**

1. 小卷积核。作者将卷积核全部替换为3x3（极少用了1x1）
2. 小池化核。相比AlexNet的3x3的池化核，VGG全部为2x2的池化核
3. 去掉了AlexNet中的局部响应归一化层（LRN）
4. 层数更深特征图更宽。基于前两点外，由于卷积核专注于扩大通道数、池化专注于缩小宽和高，使得模型架构上更深更宽的同时，计算量的增加放缓
5. 全连接转卷积。网络测试阶段将训练阶段的三个全连接替换为三个卷积，测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽或高维的输入。

**思考：**

1. 为什么VGG网络前四段里，每经过一次池化操作，卷积核个数就增加一倍

   答：

   1. 池化操作可以减小特征图尺寸，降低显存占用
   2. 增加卷积核个数有助于学习更多的结构特征，但会增加网络参数数量以及内存消耗
   3. 一减一增的设计平衡了识别精度与存储、计算开销

2. 为什么卷积核个数增加到512后就不再增加了

   答：

   1. 第一个全连接层含102M参数，占总参数个数的74%
   2. 这一层的参数个数是特征图的尺寸与个数的乘积
   3. 参数过多容易过拟合，且不易被训练

## GoogLeNet

![image-20220215161205176](D:\tools\typora\Typora-pic\image-20220215161205176.png)

**创新点：**

1. 提出了==Inception结构==，能保留输入信号中的更多特征信息

   ![image-20220215162212447](D:\tools\typora\Typora-pic\image-20220215162212447.png)

2. 去掉了AlexNet的前两个全连接层，并采用了平均池化，这一设计使得GoogLeNet只用500万参数，比AlexNet少了12倍

3. 在网络的中部引入了辅助分类器，客服了训练过程中梯度消失问题

**改进：**

1. Inception模块含有卷积操作过多，计算较慢

   解决：Inception V1

   ![image-20220215162312568](D:\tools\typora\Typora-pic\image-20220215162312568.png)

   首先由1×1卷积降低深度通道，再进行3×3或5×5卷积操作，计算速度加快

**思考：**

1. 平均池化向量化与直接展开向量化区别

   答：

   1. 特征相应图上每个位置的值反映了图像对应位置的结构与卷积核记录的语义结构的相似程度
   2. 平均池化丢失了语义结构的空间位置信息
   3. 忽略语义结构的位置信息，有助于提升卷积层提取到的特征的平移不变性

## ResNet

> 为了解决过度加深层后，出现梯度消失或梯度爆炸的现象，引入了“快捷结构”，称为残差块

![image-20220211221130237](D:\tools\typora\Typora-pic\image-20220211221130237.png)

![image-20220211214504393](D:\tools\typora\Typora-pic\image-20220211214504393.png)

> 快捷结构跳过了输入数据的卷积层，将输入x合计到输出

**贡献：**

1. 提出==残差模块==，通过堆叠残差模块可以构建任意深度的神经网络，而不会出现”退化“现象
2. 提出了==批归一化==方法来对抗梯度消失，该方法降低了网络训练过程对于权重初始化的依赖
3. 提出了一种针对ReLU激活函数的初始化方法

**作用：**

1. 残差结构能够避免普通的卷积层堆叠存在的信息丢失问题，保证前向信息流的顺畅
2. 残差结构能够应对梯度反传过程中梯度消失问题，保证反向梯度流的通顺

## R-CNN

>RCNN（Region with CNN feature）是卷积神经网络应用于目标检测问题的一个里程碑的飞跃

![image-20220216220946865](D:\tools\typora\Typora-pic\image-20220216220946865.png)

**原理：**

1. **候选区域选择：**区域建议Region Proposal是一种传统的区域提取方法，基于启发式的区域提取方法，用的方法是ss，==查看现有的小区域，合并两个最有可能的区域==，重复此步骤，直到图像合并为一个区域，最后输出候选区域。可以看作将图片分格后，以滑动窗口的方式，选取图像作为CNN输入

2. **CNN特征提取：**对每个选取的图像都做一次CNN特征提取操作

3. **分类与边界回归：**

   1. 输出向量进行分类

      常见方法为SVM，softmax

   2. 边界回归框回归（bbox）

      bbox回归，多任务损失函数边框回归

**缺点：**

1. 占用空间大
2. 计算时间过长

## Fast R-CNN

> R-CNN的改进版本

![image-20220216221136378](D:\tools\typora\Typora-pic\image-20220216221136378.png)

**提升点：**

1. **测试时的速度得到了提升。** RCNN算法与图像内的大量候选帧重叠，导致提取特征操作中的大量冗余。 而Fast RCNN则很好的解决了这一问题。
2. **训练时的速度得到了提升。**
3. **训练所需的空间大。**RCNN中分类器和回归器需要大量特征作为训练样本。而Fast RCNN则不再需要额外储存。

**原理：**

1. **选择性搜索Selective Search（SS）在图片中获得大约2k个候选框。**

2. **使用卷积网络提取图片特征：**对整个图片进行卷积，得到特征图

3. **选取特征图中对应区域：**根据之前RoI框选择出对应的区域（既可以理解为将feature map映射回原图像）， 在最后一次卷积之前，使用 RoI池层来统一相同的比例（这里利用的是单层ssp）。

   **ROI pooling层（Region of Interest）：**

   ![image-20220217195118587](D:\tools\typora\Typora-pic\image-20220217195118587.png)

   ​	ROI是指的在SS完成后得到的“候选框”在特征图上的映射；位于RoI Pooling之前

   ​	RoI Pooling层将每个候选区域分为m * n个块。 针对每个块执行最大池操作，使得特征映射上不同大小的候选区域被变换为均匀大小的特征向量，然后送入下一层。

4. **分类回归**

**缺点：**

1. Fast RCNN使用的是selective search选择性搜索，这一过程十分耗费时间，其进行候选区域提取所花费的时间约为2~3秒
2. 因为使用selective search来预先提取候选区域，Fast RCNN并没有实现真正意义上的端到端训练模式

## Faster R-CNN

**提升点：**

​	对于Faster RCNN来讲，与RCNN和Fast RCNN最大的区别就是，目标检测所需要的四个步骤，即候选区域生成，特征提取，分类器分类，回归器回归，==这四步全都交给深度神经网络来做==，并且全部运行在==GPU==上，这大大提高了操作的效率。

**模块组成：**

1. **区域生成网络RPN候选框提取模块**

   ​	RPN是全卷积神经网络，其内部与普通卷积神经网络不同之处在于是将CNN中的全连接层变成卷积层。RPN使用的方法实际上是在最后一个卷积层上滑动窗口。

   ​	RPN在m,n特征图上会产生m * n * 9个anchor，计算每个产生的anchor和标注框的交并比IoU，大于0.7为前景，小于0.3为背景，去除0.3-0.7之间的anchor。对剩下的anchor输入RoIPooling层。Roi pooling层的过程就是为了将proposal抠出来的过程，然后resize到统一的大小(14 * 14)，然后下采样到7 * 7 * 512最后输入全连接，最后进行边框精修和分类。

2. **Fast RCNN检测模块**

   Faster RCNN是基于RPN提取的proposal检测并识别proposal中的目标。

**步骤：**

1. 输入图像。
2. 通过区域生成网络RPN生成候选区域。
3. 提取特征。
4. 分类器分类。
5. 回归器回归并进行位置调整。

**对比：**

![image-20220217201603680](D:\tools\typora\Typora-pic\image-20220217201603680.png)

## Mask R-CNN

**优点：**

1. 捕捉不同尺度
2. 多样化
3. 计算快

### RPN

## YOLO

https://blog.csdn.net/wjinjie/article/details/107509243?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522164570397316780366533085%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=164570397316780366533085&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-107509243.pc_search_result_control_group&utm_term=YOLO&spm=1018.2226.3001.4187

# 疑问点

1. **神经网络为什么必须用非线性函数**

   答：如果不使用非线性函数，那么就无法发挥叠加层所带来的优势；如果不使用非线性函数，总是会存在与之等效的无隐藏层的神经网络。

2. **输出层激活函数的选取**

   答：一般而言，==回归==问题用恒等函数，==分类==问题用softmax函数。

3. **为何要设定损失函数，而不是识别精度**

   答：识别精度对微小的参数变化基本没有什么反应，即便有反应，也是不连续的、突然的变化，从而导致参数的导数再绝大多数地方都会变为0，而无法更新参数。

4. **过拟合原因**

   答：

   1. 模型拥有大量参数、表现力强
   2. 训练数据少

5. **数据分割作用**

   | 数据     | 作用                   |
   | -------- | ---------------------- |
   | 训练数据 | 用于学习、训练模型     |
   | 测试数据 | 评估泛化能力           |
   | 验证数据 | 超参数的调整和性能评估 |

6. **为什么不能用测试数据评估超参数性能？**

   答：如果使用测试数据调整超参数，超参数的值就会对测试数据发生过拟合，从而可能就会不能拟合其他数据、泛化能力低的模型

7. **CNN卷积输出大小**
   $$
   OH = \frac{H+2P-FH}{S} + 1 \\
   OW = \frac{W+2P-FW}{S} + 1
   $$
   

   输入大小（H,W）

   滤波器大小（FH,FW）

   输出大小（OH,OW）

   填充（P）

   步幅（S）

8. **卷积计算中，多个数据的批处理方式**

   基于im2col展开

   ![image-20220211175942673](D:\tools\typora\Typora-pic\image-20220211175942673.png)

   ![image-20220211175953871](D:\tools\typora\Typora-pic\image-20220211175953871.png)

9. **加深层的好处**

   1. 减少网络的参数数量，使学习更加高效
   2. 可以分层次的解决问题，分层次地传递信息

10. **大卷积核和小卷积核的区别**

    1. 越大的卷积核越擅长整合高阶的信息，小的卷积核比较擅长低阶的信息
    2. 感受野不同，大卷积核能够提取到更宽广的特征。
    3. 从理论计算上，两个3×3的卷积核是可以达到1个5×5卷积核的感受野，三个3×3的卷积核是可以达到1个7×75卷积核的感受野
    4. 使用小卷积核串联构建的网络深度更深、非线性更强、参数也更少

11. **为什么残差网络有效**

    1. 从信息传播角度

       ![image-20220211231629510](D:\tools\typora\Typora-pic\image-20220211231629510.png)

       ​	经过展开，在前向传播时，输入信号可以从任意底层直接传到高层；包含了一个天然的恒等映射，一定程度上解决了网络退化的问题

    2. 集成学习角度

       Andreas Veit提出从集成学习的视角；将残差网络展开，得到如下图

       ![image-20220211231729378](D:\tools\typora\Typora-pic\image-20220211231729378.png)

       1. **残差网络可以看作是一系列路径组装而成的一个集成模型**，不同的路径包含了不同的网络层子集。
       2. 实验表明网络的表现与正确网络路径数相关；表明残差网络展开后的路径具有一定的==独立性==和==冗余性==，使得残差网络表现的像一个集成模型
       3. **残差网络主要在训练中贡献了梯度的是那些相对较短的路径**
    
12. **在BN中，预测时均值和方差怎么求**

    答：对于预测阶段时所使用的均值和方差，其实也是来源于训练集。比如我们在模型训练时我们就记录下每个batch下的均值和方差，待训练完毕后，我们求整个训练样本的均值和方差期望值，作为我们进行预测时进行BN的的均值和方差

13. **为什么使用卷积**

    1. 训练参数少。因为参数共享和局部观察。
    2. 善于捕捉平移不变。

14. **1*1卷积核的作用**

    答：如果当前层和下一层都只有一个通道那么1×1卷积核确实没什么作用，但是如果它们分别为m层和n层的话，1×1卷积核可以起到一个==跨通道聚合==的作用，所以进一步可以起到==降维（或者升维）==的作用，起到==减少参数==的目的。 

    1. 跨通道的特征整合

    2. 特征通道的升维和降维

       ![image-20220212204856934](D:\tools\typora\Typora-pic\image-20220212204856934.png)

    3. 减少卷积核参数（简化模型）

    4. 可以看作一种全连接

       ![image-20220212204827960](D:\tools\typora\Typora-pic\image-20220212204827960.png)

    5. 跨通道信息交换
    
15. **串联结构（如VGG）存在的问题**

    答：

    ​	后面的卷积层只能处理前层输出的特征图；前层因某些原因（比如感受野限制）丢失重要信息，后层无法找回。

    ​	解决方法：Inception模块

    ​	![image-20220215161809338](D:\tools\typora\Typora-pic\image-20220215161809338.png)

16. 